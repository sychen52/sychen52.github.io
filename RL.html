<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>&lrm;</title>
<meta name="generator" content="Org mode" />
<link rel="stylesheet" type="text/css" href="https://gongzhitaao.org/orgcss/org.css"/>
</head>
<body>
<div id="content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org580aeb7">Trajectory = Episode = Rollout</a></li>
<li><a href="#org08d8e7e">Value Functions</a>
<ul>
<li><a href="#orgabd5243">On-Policy vs. Optimal</a></li>
<li><a href="#org5e2fe04">Value Function vs. Action-Value Function</a>
<ul>
<li><a href="#orgbff21fa">If On-Policy</a></li>
<li><a href="#orgcd1d44f">IF Optimal</a></li>
<li><a href="#orgd88dec4">Q-Function is Action-Value Function</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgbb178d0">Optimal action is the argmax<sub>a</sub> Optimal Q-Function</a></li>
<li><a href="#org002fb17">Bellman Equation</a></li>
<li><a href="#orgaafd60d">Advantage Functions</a></li>
<li><a href="#org7f24243">Policy Optimization vs. Q-Learning</a></li>
<li><a href="#org3f17fea">Policy Optimization and Policy Gradient</a></li>
</ul>
</div>
</div>
<div id="outline-container-org580aeb7" class="outline-2">
<h2 id="org580aeb7">Trajectory = Episode = Rollout</h2>
<div class="outline-text-2" id="text-org580aeb7">
<p>
a sequence of state and action
&tau; ~ &pi; means &tau; is a random trajectory sampled based on policy &pi;. Two things can be random here: state transition, action taken based on a policy.
</p>
</div>
</div>

<div id="outline-container-org08d8e7e" class="outline-2">
<h2 id="org08d8e7e">Value Functions</h2>
<div class="outline-text-2" id="text-org08d8e7e">
<p>
Four different types of value function, if you combine the following two.
</p>
</div>
<div id="outline-container-orgabd5243" class="outline-3">
<h3 id="orgabd5243">On-Policy vs. Optimal</h3>
<div class="outline-text-3" id="text-orgabd5243">
<p>
A specific policy &pi; vs. the optimal policy *
</p>
</div>
</div>
<div id="outline-container-org5e2fe04" class="outline-3">
<h3 id="org5e2fe04">Value Function vs. Action-Value Function</h3>
<div class="outline-text-3" id="text-org5e2fe04">
</div>
<div id="outline-container-orgbff21fa" class="outline-4">
<h4 id="orgbff21fa">If On-Policy</h4>
<div class="outline-text-4" id="text-orgbff21fa">
<p>
Expectation over all possible actions from the policy vs. given a specific action a (the action may not from the polity).
</p>
</div>
</div>
<div id="outline-container-orgcd1d44f" class="outline-4">
<h4 id="orgcd1d44f">IF Optimal</h4>
<div class="outline-text-4" id="text-orgcd1d44f">
<p>
Max over &#x2026;
</p>

<p>
Since it is the optimal policy, all possible actions only contain the best/max-return action.
</p>
</div>
</div>
<div id="outline-container-orgd88dec4" class="outline-4">
<h4 id="orgd88dec4">Q-Function is Action-Value Function</h4>
</div>
</div>
</div>

<div id="outline-container-orgbb178d0" class="outline-2">
<h2 id="orgbb178d0">Optimal action is the argmax<sub>a</sub> Optimal Q-Function</h2>
<div class="outline-text-2" id="text-orgbb178d0">
<p>
since it is optimal
</p>
</div>
</div>

<div id="outline-container-org002fb17" class="outline-2">
<h2 id="org002fb17">Bellman Equation</h2>
<div class="outline-text-2" id="text-org002fb17">
<p>
Expand any of the 4 value functions one step and make it recursive.
</p>

<p>
If it is optimal value function or Q-function, the expansion will have a max<sub>a</sub> instead of E<sub>a~&pi;</sub>.
</p>
</div>
</div>

<div id="outline-container-orgaafd60d" class="outline-2">
<h2 id="orgaafd60d">Advantage Functions</h2>
<div class="outline-text-2" id="text-orgaafd60d">
<p>
Q(s,a)-V(s)
</p>
</div>
</div>

<div id="outline-container-org7f24243" class="outline-2">
<h2 id="org7f24243">Policy Optimization vs. Q-Learning</h2>
<div class="outline-text-2" id="text-org7f24243">
<ul class="org-ul">
<li>both are for Model Free RL</li>
<li>Policy optimization predicts the distribution of the action given the observation.</li>
<li>Policy Optimization's objective/loss function estimates a [surrogate] value function
surrogate means the gradient is the same/similar, but the objective function is not just value function.</li>
<li>Q-Learning estimates Q-function based on Bellman equation</li>
<li>more vs. less stable: based Satisfy Bellman equation does not mean it is Q-function (necessary but not sufficient).</li>
<li>less vs. more data effective. (on vs. off policy). Policy Optimization has to be on the latest policy</li>
</ul>
</div>
</div>

<div id="outline-container-org3f17fea" class="outline-2">
<h2 id="org3f17fea">Policy Optimization and Policy Gradient</h2>
<div class="outline-text-2" id="text-org3f17fea">
<p>
Instead of using a NN to estimate "expected total reward of a policy" (J = E[R(&tau;)]), and then take a gradient of that and do gradient ascent, we derive the equation of the gradient of J, then ignore all the terms that has 0-grad. This new equation is called grad-log-prob.
</p>

<p>
Now the NN takes an observation and predicts the logits of &pi;(a|s). This is the parameters of the action distribution in current state. Then you sample this distribution to get an action. The so-called loss is the grad-log-prob term before taking the gradient.
</p>

<p>
You can think of this "grad-log-prob term before taking the gradient" as an approximation of J, because they have the same gradient.
</p>

<p>
In summary, NN predicts the action distribution. The loss was designed such as its gradient is the same as dJ/d&theta;.
</p>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="validation"></p>
</div>
</body>
</html>
