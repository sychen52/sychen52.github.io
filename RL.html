<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Reinforcement Learning</title>
<meta name="generator" content="Org Mode" />
<link rel="stylesheet" type="text/css" href="https://orgmode.org/worg/style/worg.css"/>
</head>
<body>
<div id="content" class="content">
<h1 class="title">Reinforcement Learning</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org1fd5355">Trajectory = Episode = Rollout</a></li>
<li><a href="#org1d58d86">Value Functions</a>
<ul>
<li><a href="#orgb7e8d2c">On-Policy vs. Optimal</a></li>
<li><a href="#orgd9d6a69">Value Function vs. Action-Value Function</a>
<ul>
<li><a href="#org2ae1051">Q-Function is Action-Value Function</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org1b69025">Optimal action is the argmax<sub>a</sub> Optimal Q-Function</a></li>
<li><a href="#org623c943">Bellman Equation</a></li>
<li><a href="#orgd503b1e">Advantage Functions</a></li>
<li><a href="#org41ae9cc">Policy Optimization vs. Q-Learning</a></li>
<li><a href="#org7ce6c05">Policy Optimization and Policy Gradient</a></li>
<li><a href="#org11271e0">Reduce sample variance</a>
<ul>
<li><a href="#org3ca38de">Reward-to-go:</a></li>
<li><a href="#org9aa41f2">Baselines</a></li>
</ul>
</li>
<li><a href="#orgea02c6b">Vanilla Policy Gradient (VPG)</a></li>
<li><a href="#org07330ad">Trust Region Policy Optimization (TRPO)</a></li>
<li><a href="#org42eb464">Proximal Policy Optimization (PPO)</a>
<ul>
<li><a href="#org1927c6a">PPO-Penalty: Similar to TRPO but the KL is in the loss function itself rather than a constraint</a></li>
<li><a href="#org01edf39">PPO-Clip</a></li>
</ul>
</li>
<li><a href="#org9b0cacd">Deep Deterministic Policy Gradient (DDPG)</a>
<ul>
<li><a href="#org1cb3054">Tricks:</a>
<ul>
<li><a href="#org39d9a5b">Experience Replay</a></li>
<li><a href="#orgf0c6d24">Target Networks</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org950c85d">Twin Delayed DDPG (TD3)</a>
<ul>
<li><a href="#org7cb32f8">Three tricks</a></li>
</ul>
</li>
<li><a href="#org853b2d4">Soft Actor Critic (SAC)</a></li>
</ul>
</div>
</div>

<div id="outline-container-org1fd5355" class="outline-2">
<h2 id="org1fd5355">Trajectory = Episode = Rollout</h2>
<div class="outline-text-2" id="text-org1fd5355">
<p>
a sequence of state and action
τ ~ π means τ is a random trajectory (s1, a1, s2, a2, &#x2026;) sampled based on policy π. Two things can be random here: state transition and the action that is taken based on a policy.
</p>
</div>
</div>

<div id="outline-container-org1d58d86" class="outline-2">
<h2 id="org1d58d86">Value Functions</h2>
<div class="outline-text-2" id="text-org1d58d86">
<p>
Four different types of value functions, if you combine the following two.
</p>
</div>
<div id="outline-container-orgb7e8d2c" class="outline-3">
<h3 id="orgb7e8d2c">On-Policy vs. Optimal</h3>
<div class="outline-text-3" id="text-orgb7e8d2c">
<p>
A specific policy π vs. the optimal policy * (Max over all policies)
</p>
</div>
</div>
<div id="outline-container-orgd9d6a69" class="outline-3">
<h3 id="orgd9d6a69">Value Function vs. Action-Value Function</h3>
<div class="outline-text-3" id="text-orgd9d6a69">
<p>
Expectation over all possible actions from the policy vs. given a specific action <code>a</code> (the action may not be from the policy).
</p>
</div>
<div id="outline-container-org2ae1051" class="outline-4">
<h4 id="org2ae1051">Q-Function is Action-Value Function</h4>
</div>
</div>
</div>

<div id="outline-container-org1b69025" class="outline-2">
<h2 id="org1b69025">Optimal action is the argmax<sub>a</sub> Optimal Q-Function</h2>
<div class="outline-text-2" id="text-org1b69025">
<p>
since it is optimal
</p>
</div>
</div>

<div id="outline-container-org623c943" class="outline-2">
<h2 id="org623c943">Bellman Equation</h2>
<div class="outline-text-2" id="text-org623c943">
<p>
Expand any of the 4 value functions one step and make it recursive.
</p>

<p>
optimal value vs. on-policy: max<sub>a</sub> vs. E<sub>a~π</sub>.
</p>
</div>
</div>

<div id="outline-container-orgd503b1e" class="outline-2">
<h2 id="orgd503b1e">Advantage Functions</h2>
<div class="outline-text-2" id="text-orgd503b1e">
<p>
Q(s,a)-V(s)
</p>
</div>
</div>

<div id="outline-container-org41ae9cc" class="outline-2">
<h2 id="org41ae9cc">Policy Optimization vs. Q-Learning</h2>
<div class="outline-text-2" id="text-org41ae9cc">
<ul class="org-ul">
<li>both are for Model Free RL</li>
<li>Policy optimization predicts the distribution of the action given the observation.</li>
<li>Policy Optimization's objective/loss function estimates a [surrogate] value function
surrogate means the gradient is the same/similar, but the objective function is not just the value function.</li>
<li>Q-Learning estimates Q-function based on the Bellman equation</li>
<li>more vs. less stable: Satisfying the Bellman equation is an indirect measure of maximizing reward. And it has failure mode.</li>
<li>less vs. more data effective. (on vs. off policy). Policy Optimization has to be on the latest policy</li>
</ul>
</div>
</div>

<div id="outline-container-org7ce6c05" class="outline-2">
<h2 id="org7ce6c05">Policy Optimization and Policy Gradient</h2>
<div class="outline-text-2" id="text-org7ce6c05">
<p>
The NN directly learns the policy itself (given state/observation as input, what should be the action (or probability of taking that action)).
</p>

<p>
Instead of computing a loss as the "expected total reward of a policy" (J = E[R(τ)]), then taking a gradient of that and do gradient ascent (This is impossible when we cannot take the derivative of the environment (p(s1), p(s2|s1, a1), &#x2026;). In model-based RL, this will be learned.), we derive the equation of the gradient of J, then ignore all the terms that have 0-grad. This new equation is called grad-log-prob.
</p>

<p>
You can think of this "grad-log-prob term before taking the gradient", namely log-prob*reward, as an approximation of J, because they have the same gradient.
</p>

<p>
Now the NN takes an observation and predicts the logits of π(a|s). These are the parameters of the action distribution in the current state. Then you sample this distribution to get an action. The so-called loss is the log-prob*reward.
</p>

<p>
However, this log-prob*reward is not a loss function. Its value goes down or up does not mean the policy is good. The only useful thing about this log-prob*reward is its gradient at the current step. 
</p>

<p>
In summary, NN predicts the action distribution. The loss was designed such as its gradient is the same as dJ/dθ.
</p>
</div>
</div>

<div id="outline-container-org11271e0" class="outline-2">
<h2 id="org11271e0">Reduce sample variance</h2>
<div class="outline-text-2" id="text-org11271e0">
</div>
<div id="outline-container-org3ca38de" class="outline-3">
<h3 id="org3ca38de">Reward-to-go:</h3>
<div class="outline-text-3" id="text-org3ca38de">
<ul class="org-ul">
<li>Causality: policy cannot affect the previous reward</li>
<li>The reason we can remove sum<sub>0</sub><sup>t-1</sup> is that its expectation is 0. In the cs285 lecture, he mentioned the proof is somewhat involved. The proof depends on EGLP lemma.</li>
</ul>
</div>
</div>
<div id="outline-container-org9aa41f2" class="outline-3">
<h3 id="org9aa41f2">Baselines</h3>
<div class="outline-text-3" id="text-org9aa41f2">
<p>
To subtract a baseline, b must have nothing to do with actions.
b can be a constant or a function of states.
The proof of b=const case is simple; however, in lecture 6, he mentioned it can also be proved if b=f(s).
</p>
</div>
</div>
</div>

<div id="outline-container-orgea02c6b" class="outline-2">
<h2 id="orgea02c6b"><a href="https://spinningup.openai.com/en/latest/algorithms/vpg.html">Vanilla Policy Gradient (VPG)</a></h2>
<div class="outline-text-2" id="text-orgea02c6b">
<ul class="org-ul">
<li>use NN to estimate policy and value function</li>
<li>update the policy NN with the gradient mentioned above (reward-to-go and value function as baseline)</li>
<li>value function NN is updated by a loss function = L2 of between predict and the real reward.</li>
</ul>
</div>
</div>

<div id="outline-container-org07330ad" class="outline-2">
<h2 id="org07330ad"><a href="https://spinningup.openai.com/en/latest/algorithms/trpo.html">Trust Region Policy Optimization (TRPO)</a></h2>
<div class="outline-text-2" id="text-org07330ad">
<ul class="org-ul">
<li>why do we want the policy before and after updating to be close.
<ul class="org-ul">
<li>cs285 lecture: TRPO and PPO derive a new "loss function". For this loss function to be the same as the one used mentioned before. There is one constraint: π<sub>θ</sub>' is close to π<sub>θ</sub> =&gt; p<sub>θ</sub>'(sₜ) is close to p<sub>θ</sub>(sₜ)</li>
<li>However, a small grad update of policy NN does NOT necessarily mean the resulting difference in policy is small (although the difference is also bound, however for some gradient direction, the resulting policy difference could be large.). This means a bad step in the policy gradient can collapse the policy</li>
</ul></li>
<li>Maximum a new surrogate advantage, with a constraint that KL divergence of the policy before and after updating is less than a threshold.
<ul class="org-ul">
<li>the new surrogate advantage has the same gradient as the one in VPG.</li>
</ul></li>
<li>Solve the Lagrangian duality.</li>
<li>The gradient update will involve the Hessian of KL.</li>
</ul>
</div>
</div>

<div id="outline-container-org42eb464" class="outline-2">
<h2 id="org42eb464"><a href="https://spinningup.openai.com/en/latest/algorithms/ppo.html#id3">Proximal Policy Optimization (PPO)</a></h2>
<div class="outline-text-2" id="text-org42eb464">
</div>
<div id="outline-container-org1927c6a" class="outline-3">
<h3 id="org1927c6a">PPO-Penalty: Similar to TRPO but the KL is in the loss function itself rather than a constraint</h3>
</div>
<div id="outline-container-org01edf39" class="outline-3">
<h3 id="org01edf39">PPO-Clip</h3>
<div class="outline-text-3" id="text-org01edf39">
<ul class="org-ul">
<li>same surrogate advantage function as TRPO, which contains a ratio between new and old policy.</li>
<li>add an extra term in loss to clip the ratio so that it does not change much.</li>
<li>They use a proxy loss to have the same gradient. This way you can use Pytorch's autograd.</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org9b0cacd" class="outline-2">
<h2 id="org9b0cacd"><a href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html">Deep Deterministic Policy Gradient (DDPG)</a></h2>
<div class="outline-text-2" id="text-org9b0cacd">
<ul class="org-ul">
<li>DDPG is an offline algorithm that learns the optimal deterministic policy and the Q*.
<ul class="org-ul">
<li>VPG, TRPO, and PPO are all online algorithms that learn a policy and the value function for that policy</li>
</ul></li>
<li>DDPG is the Q-learning for continuous space.
<ul class="org-ul">
<li>Getting the optimal policy from Q* is trivial for finite discretized action space (Thus, Q-learning is enough.). However, for continuous action space, it is not easy. This is why DDPG learns the optimal policy instead of computing it based on Q*.</li>
<li>To learn this optimal policy, to loss function is just to max Q* (while the parameters in the Q* NN are fixed.).</li>
</ul></li>
</ul>
</div>
<div id="outline-container-org1cb3054" class="outline-3">
<h3 id="org1cb3054">Tricks:</h3>
<div class="outline-text-3" id="text-org1cb3054">
</div>
<div id="outline-container-org39d9a5b" class="outline-4">
<h4 id="org39d9a5b">Experience Replay</h4>
<div class="outline-text-4" id="text-org39d9a5b">
<ul class="org-ul">
<li>For previous on-policy algorithms, such as VPG, TRPO, and PPO. The loss function and the gradient term have an expectation term sample over the current policy. This makes the old experience unusable.</li>
<li>For the Q-learning algorithm, the loss function is mean-squared Bellman error (MSBE), and this equation should be satisfied with any action you sampled.</li>
</ul>
</div>
</div>
<div id="outline-container-orgf0c6d24" class="outline-4">
<h4 id="orgf0c6d24">Target Networks</h4>
<div class="outline-text-4" id="text-orgf0c6d24">
<ul class="org-ul">
<li>There is a "max Q*" term inside the Bellman equation.
<ul class="org-ul">
<li>As mentioned above, to estimate the max part in a continuous action space, you need to use the estimated optimal policy.</li>
<li>if both Q* in the Bellman equation are the same Q* estimated by NN, it will be unstable. Therefore, they use a target network for Q* and a target network for optimal policy. These two are used for estimating "max Q*". The target network is just a running average of the normal NN.</li>
</ul></li>
</ul>
</div>
</div>
</div>
</div>

<div id="outline-container-org950c85d" class="outline-2">
<h2 id="org950c85d"><a href="https://spinningup.openai.com/en/latest/algorithms/td3.html">Twin Delayed DDPG (TD3)</a></h2>
<div class="outline-text-2" id="text-org950c85d">
</div>
<div id="outline-container-org7cb32f8" class="outline-3">
<h3 id="org7cb32f8">Three tricks</h3>
<div class="outline-text-3" id="text-org7cb32f8">
<ul class="org-ul">
<li>learn 2 Q-function and use the smaller of the two Q-value (because Q-value tend to over-estimate [cs285 lecture 8])</li>
<li>update policy less frequently than Q-function</li>
<li>add noise to target action to smooth the policy (Q over-estimate)</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org853b2d4" class="outline-2">
<h2 id="org853b2d4">Soft Actor Critic (SAC)</h2>
<div class="outline-text-2" id="text-org853b2d4">
<ul class="org-ul">
<li>similar to TD3
<ul class="org-ul">
<li>learn a policy and two Q-functions.</li>
<li>add an entropy regularization of policy into Q function estimation. Basically, encourage more random policy.</li>
</ul></li>
</ul>


<script src="https://utteranc.es/client.js"
        repo="sychen52/sychen52.github.io"
        issue-term="pathname"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>
</div>
</div>
</div>
</body>
</html>
