<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Reinforcement Learning</title>
<meta name="generator" content="Org mode" />
<link rel="stylesheet" type="text/css" href="https://orgmode.org/worg/style/worg.css"/>
</head>
<body>
<div id="content">
<h1 class="title">Reinforcement Learning</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orged73507">Trajectory = Episode = Rollout</a></li>
<li><a href="#org5cba54d">Value Functions</a>
<ul>
<li><a href="#org12c18f1">On-Policy vs. Optimal</a></li>
<li><a href="#org0c3c6d5">Value Function vs. Action-Value Function</a>
<ul>
<li><a href="#org6a592db">Q-Function is Action-Value Function</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orge6d2312">Optimal action is the argmax<sub>a</sub> Optimal Q-Function</a></li>
<li><a href="#org43106bd">Bellman Equation</a></li>
<li><a href="#orgf28dfa0">Advantage Functions</a></li>
<li><a href="#org98db71f">Policy Optimization vs. Q-Learning</a></li>
<li><a href="#orgf87296d">Policy Optimization and Policy Gradient</a></li>
<li><a href="#org48007f8">Reduce sample variance</a>
<ul>
<li><a href="#org30a1136">Reward-to-go:</a></li>
<li><a href="#orgd92ea7d">Baselines</a></li>
</ul>
</li>
<li><a href="#org1d60edd">Vinilla Policy Gradient (VPG)</a></li>
<li><a href="#org84ab67f">Trust Region Policy Optimization (TRPO)</a></li>
<li><a href="#org67ff14f">Proximal Policy Optimization (PPO)</a>
<ul>
<li><a href="#org11169d3">PPO-Penalty: Similar to TRPO but the KL is in the loss function itself rather than a constraint</a></li>
<li><a href="#orga16b8b8">PPO-Clip</a></li>
<li><a href="#orgc7aaa59">Tricks:</a>
<ul>
<li><a href="#org3a0bbf6">Experience Replay</a></li>
<li><a href="#org3e151ca">Target Networks</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org4770fb9">Twin Delayed DDPG (TD3)</a>
<ul>
<li><a href="#org8e6bad8">Three tricks</a></li>
</ul>
</li>
<li><a href="#orgc6bb67b">Soft Actor Critic (SAC)</a></li>
</ul>
</div>
</div>

<div id="outline-container-orged73507" class="outline-2">
<h2 id="orged73507">Trajectory = Episode = Rollout</h2>
<div class="outline-text-2" id="text-orged73507">
<p>
a sequence of state and action
τ ~ π means τ is a random trajectory (s1, a1, s2, a2, &#x2026;) sampled based on policy π. Two things can be random here: state transition and the action that is taken based on a policy.
</p>
</div>
</div>

<div id="outline-container-org5cba54d" class="outline-2">
<h2 id="org5cba54d">Value Functions</h2>
<div class="outline-text-2" id="text-org5cba54d">
<p>
Four different types of value functions, if you combine the following two.
</p>
</div>
<div id="outline-container-org12c18f1" class="outline-3">
<h3 id="org12c18f1">On-Policy vs. Optimal</h3>
<div class="outline-text-3" id="text-org12c18f1">
<p>
A specific policy π vs. the optimal policy * (Max over all policies)
</p>
</div>
</div>
<div id="outline-container-org0c3c6d5" class="outline-3">
<h3 id="org0c3c6d5">Value Function vs. Action-Value Function</h3>
<div class="outline-text-3" id="text-org0c3c6d5">
<p>
Expectation over all possible actions from the policy vs. given a specific action <code>a</code> (the action may not be from the policy).
</p>
</div>
<div id="outline-container-org6a592db" class="outline-4">
<h4 id="org6a592db">Q-Function is Action-Value Function</h4>
</div>
</div>
</div>

<div id="outline-container-orge6d2312" class="outline-2">
<h2 id="orge6d2312">Optimal action is the argmax<sub>a</sub> Optimal Q-Function</h2>
<div class="outline-text-2" id="text-orge6d2312">
<p>
since it is optimal
</p>
</div>
</div>

<div id="outline-container-org43106bd" class="outline-2">
<h2 id="org43106bd">Bellman Equation</h2>
<div class="outline-text-2" id="text-org43106bd">
<p>
Expand any of the 4 value functions one step and make it recursive.
</p>

<p>
optimal value vs. on-policy: max<sub>a</sub> vs. E<sub>a~π</sub>.
</p>
</div>
</div>

<div id="outline-container-orgf28dfa0" class="outline-2">
<h2 id="orgf28dfa0">Advantage Functions</h2>
<div class="outline-text-2" id="text-orgf28dfa0">
<p>
Q(s,a)-V(s)
</p>
</div>
</div>

<div id="outline-container-org98db71f" class="outline-2">
<h2 id="org98db71f">Policy Optimization vs. Q-Learning</h2>
<div class="outline-text-2" id="text-org98db71f">
<ul class="org-ul">
<li>both are for Model Free RL</li>
<li>Policy optimization predicts the distribution of the action given the observation.</li>
<li>Policy Optimization's objective/loss function estimates a [surrogate] value function
surrogate means the gradient is the same/similar, but the objective function is not just the value function.</li>
<li>Q-Learning estimates Q-function based on the Bellman equation</li>
<li>more vs. less stable: Satisfying the Bellman equation is an indirect measure of maximizing reward. And it has failure mode.</li>
<li>less vs. more data effective. (on vs. off policy). Policy Optimization has to be on the latest policy</li>
</ul>
</div>
</div>

<div id="outline-container-orgf87296d" class="outline-2">
<h2 id="orgf87296d">Policy Optimization and Policy Gradient</h2>
<div class="outline-text-2" id="text-orgf87296d">
<p>
The NN directly learns the policy itself (given state/observation as input, what should be the action (or probability of taking that action)).
</p>

<p>
Instead of computing a loss as the "expected total reward of a policy" (J = E[R(τ)]), then taking a gradient of that and do gradient ascent (This is impossible when we cannot take the derivative of the environment (p(s1), p(s2|s1, a1), &#x2026;). In model-based RL, this will be learned.), we derive the equation of the gradient of J, then ignore all the terms that have 0-grad. This new equation is called grad-log-prob.
</p>

<p>
You can think of this "grad-log-prob term before taking the gradient", namingly log-prob*reward, as an approximation of J, because they have the same gradient.
</p>

<p>
Now the NN takes an observation and predicts the logits of π(a|s). These are the parameters of the action distribution in the current state. Then you sample this distribution to get an action. The so-called loss is the log-prob*reward.
</p>

<p>
However, this log-prob*reward is not a loss function. Its value goes down or up does not mean the policy is good. The only useful thing about this log-prob*reward is its gradient at the current step. 
</p>

<p>
In summary, NN predicts the action distribution. The loss was designed such as its gradient is the same as dJ/dθ.
</p>
</div>
</div>

<div id="outline-container-org48007f8" class="outline-2">
<h2 id="org48007f8">Reduce sample variance</h2>
<div class="outline-text-2" id="text-org48007f8">
</div>
<div id="outline-container-org30a1136" class="outline-3">
<h3 id="org30a1136">Reward-to-go:</h3>
<div class="outline-text-3" id="text-org30a1136">
<ul class="org-ul">
<li>Causality: policy cannot affect the previous reward</li>
<li>The reason we can remove sum<sub>0</sub><sup>t-1</sup> is that its expectation is 0. In the cs285 lecture, he mentioned the proof is somewhat involved. The proof depends on EGLP lemma.</li>
</ul>
</div>
</div>
<div id="outline-container-orgd92ea7d" class="outline-3">
<h3 id="orgd92ea7d">Baselines</h3>
<div class="outline-text-3" id="text-orgd92ea7d">
<p>
To subtract a baseline, b must have nothing to do with actions.
b can be a constant or a function of states.
The proof of b=const case is simple; however, in lecture 6, he mentioned it can also be proved if b=f(s).
</p>
</div>
</div>
</div>

<div id="outline-container-org1d60edd" class="outline-2">
<h2 id="org1d60edd"><a href="https://spinningup.openai.com/en/latest/algorithms/vpg.html">Vinilla Policy Gradient (VPG)</a></h2>
<div class="outline-text-2" id="text-org1d60edd">
<ul class="org-ul">
<li>use NN to estimate policy and value function</li>
<li>update the policy NN with the gradient mentioned above (reward-to-go and value functin as baseline)</li>
<li>value function NN is updated by a loss function = L2 of between predict and the real reward.</li>
</ul>
</div>
</div>

<div id="outline-container-org84ab67f" class="outline-2">
<h2 id="org84ab67f"><a href="https://spinningup.openai.com/en/latest/algorithms/trpo.html">Trust Region Policy Optimization (TRPO)</a></h2>
<div class="outline-text-2" id="text-org84ab67f">
<ul class="org-ul">
<li>why do we want the policy before and after updating to be close.
<ul class="org-ul">
<li>A small grad update of policy NN does NOT necessarily mean the resulting difference in policy is small. This means a bad step in the policy gradient can collapse the policy</li>
<li>cs285 lecture: TRPO and PPO have a new "loss function". For this loss function to be the same as the one used mentioned before. There is one constraint: π<sub>θ</sub>' is close to π<sub>θ</sub> =&gt; p<sub>θ</sub>'(sₜ) is close to p<sub>θ</sub>(sₜ)</li>
</ul></li>
<li>Maximum a new surrogate advantage, with a constraint that KL divergence of the policy before and after updating is less than a threshold.
<ul class="org-ul">
<li>the new surrogate advantage has the same gradient as the one in VPG.</li>
</ul></li>
<li>Solve the Lagrangian duality.</li>
<li>The gradient update will involve the Hessian of KL.</li>
</ul>
</div>
</div>

<div id="outline-container-org67ff14f" class="outline-2">
<h2 id="org67ff14f"><a href="https://spinningup.openai.com/en/latest/algorithms/ppo.html#id3">Proximal Policy Optimization (PPO)</a></h2>
<div class="outline-text-2" id="text-org67ff14f">
</div>
<div id="outline-container-org11169d3" class="outline-3">
<h3 id="org11169d3">PPO-Penalty: Similar to TRPO but the KL is in the loss function itself rather than a constraint</h3>
</div>
<div id="outline-container-orga16b8b8" class="outline-3">
<h3 id="orga16b8b8">PPO-Clip</h3>
<div class="outline-text-3" id="text-orga16b8b8">
<ul class="org-ul">
<li>same surrogate advantage function as TRPO, which contains a ratio between new and old policy.</li>
<li>add an extra term in loss to clip the ratio so that it does not change much.</li>
<li>The question I have: they did not compute the gradient of the loss here. Maybe they use autograd here?
<ul class="org-ul">
<li>Answer: I think so, you need a proxy loss to have the same gradient. This way you can use Pytorch's autograd.</li>
</ul></li>
</ul>

<p>
<a href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html">Deep Deterministic Policy Gradient (DDPG)</a>
</p>
<ul class="org-ul">
<li>DDPG is an offline algorithm that learns the optimal deterministic policy and the Q*.
<ul class="org-ul">
<li>VPG, TRPO, and PPO are all online algorithms that learn a policy and the value function for that policy</li>
</ul></li>
<li>DDPG is the Q-learning for continuous space.
<ul class="org-ul">
<li>Getting the optimal policy from Q* is trivial for finite discretized action space (Thus, Q-learning is enough.). However, for continuous action space, it is not easy. This is why DDPG learns the optimal policy instead of computing it based on Q*.</li>
<li>To learn this optimal policy, to loss function is just to max Q* (while the parameters in the Q* NN are fixed.).</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgc7aaa59" class="outline-3">
<h3 id="orgc7aaa59">Tricks:</h3>
<div class="outline-text-3" id="text-orgc7aaa59">
</div>
<div id="outline-container-org3a0bbf6" class="outline-4">
<h4 id="org3a0bbf6">Experience Replay</h4>
<div class="outline-text-4" id="text-org3a0bbf6">
<ul class="org-ul">
<li>For previous on-policy algorithms, such as VPG, TRPO, and PPO. The loss function and the gradient term have an expectation term sample over the current policy. This makes the old experience unusable.</li>
<li>For the Q-learning algorithm, the loss function is mean-squared Bellman error (MSBE), and this equation should be satisfied with any action you sampled.</li>
</ul>
</div>
</div>
<div id="outline-container-org3e151ca" class="outline-4">
<h4 id="org3e151ca">Target Networks</h4>
<div class="outline-text-4" id="text-org3e151ca">
<ul class="org-ul">
<li>There is a "max Q*" term inside the Bellman equation.
<ul class="org-ul">
<li>As mentioned above, to estimate the max part in a continuous action space, you need to use the estimated optimal policy.</li>
<li>if both Q* in the Bellman equation are the same Q* estimated by NN, it will be unstable. Therefore, they use a target network for Q* and a target network for optimal policy. These two are used for estimating "max Q*". The target network is just a running average of the normal NN.</li>
</ul></li>
</ul>
</div>
</div>
</div>
</div>

<div id="outline-container-org4770fb9" class="outline-2">
<h2 id="org4770fb9"><a href="https://spinningup.openai.com/en/latest/algorithms/td3.html">Twin Delayed DDPG (TD3)</a></h2>
<div class="outline-text-2" id="text-org4770fb9">
</div>
<div id="outline-container-org8e6bad8" class="outline-3">
<h3 id="org8e6bad8">Three tricks</h3>
<div class="outline-text-3" id="text-org8e6bad8">
<ul class="org-ul">
<li>learn 2 Q-function and use the smaller of the two Q-value (because Q-value tend to over-estimate [cs285 lecture 8])</li>
<li>update policy less frequently than Q-function</li>
<li>add noise to target action to smooth the policy (Q over-estimate)</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgc6bb67b" class="outline-2">
<h2 id="orgc6bb67b">Soft Actor Critic (SAC)</h2>
<div class="outline-text-2" id="text-orgc6bb67b">
<ul class="org-ul">
<li>similar to TD3
<ul class="org-ul">
<li>learn a policy and two Q-functions.</li>
<li>add an entropy regularization of policy into Q function estimation. Basically, encourage more random policy.</li>
</ul></li>
</ul>


<script src="https://utteranc.es/client.js"
        repo="sychen52/sychen52.github.io"
        issue-term="pathname"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="validation"></p>
</div>
</body>
</html>
