<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>&lrm;</title>
<meta name="generator" content="Org mode" />
<link rel="stylesheet" type="text/css" href="https://orgmode.org/worg/style/worg.css"/>
</head>
<body>
<div id="content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org0e1d42b">Trajectory = Episode = Rollout</a></li>
<li><a href="#org1260fbc">Value Functions</a>
<ul>
<li><a href="#orgd527b33">On-Policy vs. Optimal</a></li>
<li><a href="#org01d38c7">Value Function vs. Action-Value Function</a>
<ul>
<li><a href="#org5d65edb">If On-Policy</a></li>
<li><a href="#orga9d0113">IF Optimal</a></li>
<li><a href="#org0100620">Q-Function is Action-Value Function</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgaa0aaeb">Optimal action is the argmax<sub>a</sub> Optimal Q-Function</a></li>
<li><a href="#org6fc1f66">Bellman Equation</a></li>
<li><a href="#org300ae60">Advantage Functions</a></li>
<li><a href="#org4e229ec">Policy Optimization vs. Q-Learning</a></li>
<li><a href="#orgec6dad4">Policy Optimization and Policy Gradient</a></li>
</ul>
</div>
</div>
<p>
#+title Reinforcement Learning
</p>

<div id="outline-container-org0e1d42b" class="outline-2">
<h2 id="org0e1d42b">Trajectory = Episode = Rollout</h2>
<div class="outline-text-2" id="text-org0e1d42b">
<p>
a sequence of state and action
&tau; ~ &pi; means &tau; is a random trajectory sampled based on policy &pi;. Two things can be random here: state transition, action taken based on a policy.
</p>
</div>
</div>

<div id="outline-container-org1260fbc" class="outline-2">
<h2 id="org1260fbc">Value Functions</h2>
<div class="outline-text-2" id="text-org1260fbc">
<p>
Four different types of value function, if you combine the following two.
</p>
</div>
<div id="outline-container-orgd527b33" class="outline-3">
<h3 id="orgd527b33">On-Policy vs. Optimal</h3>
<div class="outline-text-3" id="text-orgd527b33">
<p>
A specific policy &pi; vs. the optimal policy *
</p>
</div>
</div>
<div id="outline-container-org01d38c7" class="outline-3">
<h3 id="org01d38c7">Value Function vs. Action-Value Function</h3>
<div class="outline-text-3" id="text-org01d38c7">
</div>
<div id="outline-container-org5d65edb" class="outline-4">
<h4 id="org5d65edb">If On-Policy</h4>
<div class="outline-text-4" id="text-org5d65edb">
<p>
Expectation over all possible actions from the policy vs. given a specific action a (the action may not from the polity).
</p>
</div>
</div>
<div id="outline-container-orga9d0113" class="outline-4">
<h4 id="orga9d0113">IF Optimal</h4>
<div class="outline-text-4" id="text-orga9d0113">
<p>
Max over &#x2026;
</p>

<p>
Since it is the optimal policy, all possible actions only contain the best/max-return action.
</p>
</div>
</div>
<div id="outline-container-org0100620" class="outline-4">
<h4 id="org0100620">Q-Function is Action-Value Function</h4>
</div>
</div>
</div>

<div id="outline-container-orgaa0aaeb" class="outline-2">
<h2 id="orgaa0aaeb">Optimal action is the argmax<sub>a</sub> Optimal Q-Function</h2>
<div class="outline-text-2" id="text-orgaa0aaeb">
<p>
since it is optimal
</p>
</div>
</div>

<div id="outline-container-org6fc1f66" class="outline-2">
<h2 id="org6fc1f66">Bellman Equation</h2>
<div class="outline-text-2" id="text-org6fc1f66">
<p>
Expand any of the 4 value functions one step and make it recursive.
</p>

<p>
If it is optimal value function or Q-function, the expansion will have a max<sub>a</sub> instead of E<sub>a~&pi;</sub>.
</p>
</div>
</div>

<div id="outline-container-org300ae60" class="outline-2">
<h2 id="org300ae60">Advantage Functions</h2>
<div class="outline-text-2" id="text-org300ae60">
<p>
Q(s,a)-V(s)
</p>
</div>
</div>

<div id="outline-container-org4e229ec" class="outline-2">
<h2 id="org4e229ec">Policy Optimization vs. Q-Learning</h2>
<div class="outline-text-2" id="text-org4e229ec">
<ul class="org-ul">
<li>both are for Model Free RL</li>
<li>Policy optimization predicts the distribution of the action given the observation.</li>
<li>Policy Optimization's objective/loss function estimates a [surrogate] value function
surrogate means the gradient is the same/similar, but the objective function is not just value function.</li>
<li>Q-Learning estimates Q-function based on Bellman equation</li>
<li>more vs. less stable: Satisfying Bellman equation is an indirect meassure of maximizing reward. And it has failure mode.</li>
<li>less vs. more data effective. (on vs. off policy). Policy Optimization has to be on the latest policy</li>
</ul>
</div>
</div>

<div id="outline-container-orgec6dad4" class="outline-2">
<h2 id="orgec6dad4">Policy Optimization and Policy Gradient</h2>
<div class="outline-text-2" id="text-orgec6dad4">
<p>
This time, the NN directly learn the policy itself (given state/observation as input, what should be the action (or probability of take that action)).
</p>

<p>
Instead of computing a loss as "expected total reward of a policy" (J = E[R(&tau;)]), and then take a gradient of that and do gradient ascent (This is impossible when we cannot take derivative of the environment), we derive the equation of the gradient of J, then ignore all the terms that has 0-grad. This new equation is called grad-log-prob.
</p>

<p>
Now the NN takes an observation and predicts the logits of &pi;(a|s). This is the parameters of the action distribution in current state. Then you sample this distribution to get an action. The so-called loss is the grad-log-prob term before taking the gradient.
</p>

<p>
You can think of this "grad-log-prob term before taking the gradient" as an approximation of J, because they have the same gradient.
</p>

<p>
In summary, NN predicts the action distribution. The loss was designed such as its gradient is the same as dJ/d&theta;.
</p>

<script src="https://utteranc.es/client.js"
        repo="sychen52/sychen52.github.io"
        issue-term="pathname"
        theme="github"
        crossorigin="anonymous"
        async>
</script>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="validation"></p>
</div>
</body>
</html>
