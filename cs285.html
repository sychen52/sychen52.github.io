<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Notes of CS285: Deep Reinforcement Learning</title>
<meta name="generator" content="Org mode" />
<link rel="stylesheet" type="text/css" href="https://orgmode.org/worg/style/worg.css"/>
</head>
<body>
<div id="content">
<h1 class="title">Notes of CS285: Deep Reinforcement Learning</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgead8da1">The loss function for imitation learning (behavior clone and Dagger)</a>
<ul>
<li><a href="#org2fe5dca"><code>MSELoss()(dist.rsample(), expert_action)</code></a></li>
<li><a href="#orgfbf91b7"><code>-dist.log_prob().sum(-1).mean()</code></a></li>
</ul>
</li>
<li><a href="#org9ac2b5b">Distribution</a>
<ul>
<li><a href="#orgfdf121b">Discrete</a></li>
<li><a href="#org16f1aa1">Continuous</a></li>
<li><a href="#orge8227bf">Normal as indenpendent Gaussian</a></li>
</ul>
</li>
<li><a href="#orgc1429b1">Behaviour Clone (BC) vs. Dagger</a>
<ul>
<li><a href="#org8ae1244">control total training steps and the model the same</a></li>
<li><a href="#orgcc0b29e">also control the total amount of data the same</a></li>
</ul>
</li>
<li><a href="#orga8346cb">Discount factor &gamma;</a></li>
<li><a href="#org5909edd">Causality: policy cannot affect the previous reward</a></li>
<li><a href="#orgf1b72f8">Baselines in advantage</a></li>
<li><a href="#orgf83ecd6">Multithread and Multiprocess</a></li>
<li><a href="#orgd99f7b5">Fitted Q/value-iteration and Actor-Critic do not guarantee to converge.</a></li>
<li><a href="#orgaab8c32">Make sample<sub>trajactory</sub> multithread / multiprocess</a></li>
</ul>
</div>
</div>

<div id="outline-container-orgead8da1" class="outline-2">
<h2 id="orgead8da1">The loss function for imitation learning (behavior clone and Dagger)</h2>
<div class="outline-text-2" id="text-orgead8da1">
</div>
<div id="outline-container-org2fe5dca" class="outline-3">
<h3 id="org2fe5dca"><code>MSELoss()(dist.rsample(), expert_action)</code></h3>
<div class="outline-text-3" id="text-org2fe5dca">
<p>
I so this solution <a href="https://github.com/berkeleydeeprlcourse/homework_fall2021/compare/main...ChoiJangho:main">here</a>. I feel this might be the intended solution because in the provided code, there is already a self.loss=MSELoss()
</p>
</div>
</div>
<div id="outline-container-orgfbf91b7" class="outline-3">
<h3 id="orgfbf91b7"><code>-dist.log_prob().sum(-1).mean()</code></h3>
<div class="outline-text-3" id="text-orgfbf91b7">
<ul class="org-ul">
<li>However, I think this is a better loss because it does not need the exact unnecessary randomness in <code>dist.rsample()</code>. As a piece of evidence, I notice this loss will yield a better Eval<sub>AverageReturn</sub> with shorter training steps and a smaller training batch size.</li>
<li>sum(-1) is to multiply 8 independent gaussian to make a joint Gaussian distribution; mean() instead of sum() so that the gradient contribution of each batch is the same no matter what is the batch<sub>size</sub>. sum() will make each sample's contribution the same, so a larger batch<sub>size</sub> will have a greater contribution.</li>
</ul>
</div>
</div>
</div>


<div id="outline-container-org9ac2b5b" class="outline-2">
<h2 id="org9ac2b5b">Distribution</h2>
<div class="outline-text-2" id="text-org9ac2b5b">
</div>
<div id="outline-container-orgfdf121b" class="outline-3">
<h3 id="orgfdf121b">Discrete</h3>
<div class="outline-text-3" id="text-orgfdf121b">
<p>
prob: [0, 1]
log<sub>prob</sub>: [-inf, 0]
</p>
</div>
</div>
<div id="outline-container-org16f1aa1" class="outline-3">
<h3 id="org16f1aa1">Continuous</h3>
<div class="outline-text-3" id="text-org16f1aa1">
<p>
prob==likelihood: [0, +inf]
log<sub>prob</sub>: [-inf, +inf]
</p>
</div>
</div>
<div id="outline-container-orge8227bf" class="outline-3">
<h3 id="orge8227bf">Normal as indenpendent Gaussian</h3>
<div class="outline-text-3" id="text-orge8227bf">
<ul class="org-ul">
<li>multiply all the prob or sum all the log<sub>prob</sub></li>
</ul>
</div>
</div>
</div>


<div id="outline-container-orgc1429b1" class="outline-2">
<h2 id="orgc1429b1">Behaviour Clone (BC) vs. Dagger</h2>
<div class="outline-text-2" id="text-orgc1429b1">
</div>
<div id="outline-container-org8ae1244" class="outline-3">
<h3 id="org8ae1244">control total training steps and the model the same</h3>
<div class="outline-text-3" id="text-org8ae1244">
<ul class="org-ul">
<li>For easy experiments like Ant, Dagger is just slightly better than BC (I keep the total training steps the same for BC and Dagger).</li>
<li>For hard experiments like Humanoid, Dagger is just 2x better than BC (I keep the total training steps the same for BC and Dagger). I increased the training iteration another 5x (also the amount of data labeled by expert 5x), Dagger is more than 10x better than BC.</li>
<li>I think this is partly because Dagger sees much more various data than BC</li>
</ul>
</div>
</div>
<div id="outline-container-orgcc0b29e" class="outline-3">
<h3 id="orgcc0b29e">also control the total amount of data the same</h3>
<div class="outline-text-3" id="text-orgcc0b29e">
<ul class="org-ul">
<li>For easy experiments like Ant, Dagger is just slightly better than BC (I keep the total training steps the same for BC and Dagger).</li>
<li>For hard experiments like Humanoid, it is too hard to train a good model with just 2000 data samples (The amount of expert data I have for BC).</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orga8346cb" class="outline-2">
<h2 id="orga8346cb">Discount factor &gamma;</h2>
<div class="outline-text-2" id="text-orga8346cb">
<p>
The effect is that it modifies the MDP by adding a death state. And 1 - &gamma; is the probability of transiting into the death state.
</p>
</div>
</div>

<div id="outline-container-org5909edd" class="outline-2">
<h2 id="org5909edd">Causality: policy cannot affect the previous reward</h2>
<div class="outline-text-2" id="text-org5909edd">
<p>
The reason we can remove sum<sub>0</sub><sup>t-1</sup> is that its expectation is 0. In the lecture, he mentioned the proof is somewhat involved. However, I think there must be some additional requirements for the reward, e.g. centered.
</p>
</div>
</div>

<div id="outline-container-orgf1b72f8" class="outline-2">
<h2 id="orgf1b72f8">Baselines in advantage</h2>
<div class="outline-text-2" id="text-orgf1b72f8">
<p>
To take of b, b must have nothing to do with actions.
b can be a constant or a function of states.
The proof of b=const case is simple; however, in lecture 6, he mentioned it can also be proved if b=f(s).
</p>
</div>
</div>

<div id="outline-container-orgf83ecd6" class="outline-2">
<h2 id="orgf83ecd6">Multithread and Multiprocess</h2>
<div class="outline-text-2" id="text-orgf83ecd6">
<p>
hw2 bonus question is to make sample<sub>trajectories</sub> multithread. However, after multi-threading, it is much slower (the more threads, the slower). I guess, 1) something under the hood does not release GIL, for example, gym? 2) it is already fast on my machine &lt; 1ms, so the overhead of thread is more significant.
Multiprocess does not work because it uses fork, and CUDA does not work on fork.
P.S. batch<sub>size</sub> = 50k
</p>
</div>
</div>

<div id="outline-container-orgd99f7b5" class="outline-2">
<h2 id="orgd99f7b5">Fitted Q/value-iteration and Actor-Critic do not guarantee to converge.</h2>
<div class="outline-text-2" id="text-orgd99f7b5">
<ul class="org-ul">
<li>Q/value-iteration (Bellman equation part) converges</li>
<li>Deep learning fits the Q/value function part also converges</li>
<li>However, the combination of these two no longer guarantees to converge.</li>
<li>For the same reason, the critic part in AC does not guarantee to converge either.</li>
</ul>
</div>
</div>

<div id="outline-container-orgaab8c32" class="outline-2">
<h2 id="orgaab8c32">Make sample<sub>trajactory</sub> multithread / multiprocess</h2>
<div class="outline-text-2" id="text-orgaab8c32">
<ul class="org-ul">
<li>before multithread / multiprocess, only one CPU is used at 100%. Even without setting the OPENBLAS<sub>NUM</sub><sub>THREADS</sub>=1.</li>
<li>multithread works; however, all CPUs work at ~10%, and the total time is 3x longer than single thread.</li>
<li>multiprocess crashes (because of the way torch uses CUDA. Some extra work is needed to get this work.); however, all CPUs go to 100% before crash.</li>
<li>I think something (env or torch) does not release GIL.</li>
</ul>

<script src="https://utteranc.es/client.js"
        repo="sychen52/sychen52.github.io"
        issue-term="pathname"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="validation"></p>
</div>
</body>
</html>
